import mediapipe as mp
import cv2
import math
import osascript
import numpy as np

Hands = mp.solutions.hands.Hands(min_detection_confidence=0.7)
draw = mp.solutions.drawing_utils
cap = cv2.VideoCapture(0)
hNo = 0

while True:
    success, img = cap.read()
    if success:
        img = cv2.flip(img, 1)
        imgRGB = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        results = Hands.process(imgRGB)
        lmList = []
        if results.multi_hand_landmarks:
            for handLMS in results.multi_hand_landmarks:
                for id, lm in enumerate(handLMS.landmark):
                    h, w, c = img.shape
                    lmList.append([id, int(lm.x * w), int(lm.y * h)])
                    draw.draw_landmarks(img, handLMS, mp.solutions.hands.HAND_CONNECTIONS)
                if len(lmList) != 0:
                    x2 = lmList[4]
                    x1 = lmList[8]
                    dist = math.sqrt(pow(x2[1] - x1[1], 2) + pow(x2[2] - x1[2], 2))
                    cv2.line(img, (x1[1], x1[2]), (x2[1], x2[2]), (255, 0, 255), 2)

                    #setting volume on macOS using osascript
                    target_volume = np.interp(dist, [50, 250], [0, 100])
                    osascript.osascript("set volume output volume {}".format(target_volume))

        cv2.imshow("Webcam input", img)
        if cv2.waitKey(1) & 0xFF == ord('q'):
            break
    else:
        break
